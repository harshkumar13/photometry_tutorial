{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YR26CKYW2YV9"
   },
   "source": [
    "# Basics of Image reduction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rxJnBx6Y2YWB"
   },
   "source": [
    "**Tutorial prepared by:**<br>\n",
    "**Harsh Kumar** (Graduate student @ Indian Institute of Techonology, Bombay (IITB), LSSTC Data Science Fellow)<br>\n",
    "**Gaurav Waratkar** (Graduate student @ Indian Institute of Techonology, Bombay (IITB))<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rg_KhkSn2YWC"
   },
   "source": [
    "## Main Motive\n",
    "Understanding the standard telescope data and making it ready for further science analysis. This step is usually termed as calibraing the pre-processing of the data.\n",
    "\n",
    "## Key steps\n",
    "- Understanding the data acquisition.\n",
    "- Handling fits files.\n",
    "- Pre-processing RAW images using bias and flat fields. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wOMYP0J92YWD"
   },
   "source": [
    "**A few important things before we get started:-**\n",
    "- python3 environment is recommended for this notebook with the following modeules insatlled: (you can also make use of conda to make such an environment.)\n",
    "- numpy\n",
    "- matplotlib\n",
    "- astropy\n",
    "- photutils\n",
    "\n",
    "If any of these modules are not installed, a simple pip insatll might do the job. i.e.  `pip install <module>`. You can also use conda to install these modules if you are working in a conda environment. If you are working with conda environment, you might want to make sure that your environment is active and pip is insatlled within your working conda environment to your conda environment\n",
    "\n",
    "**We also require a few additional astrometic software dependency :-**\n",
    "- SExtractor   https://www.astromatic.net/software"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hiVnNzIh2YWD"
   },
   "source": [
    "## Let's get started!\n",
    "We will first import all necessary modules and do some important checks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSiKAeD_2YWE"
   },
   "source": [
    "! pip install astroquery\n",
    "! pip install astroscrappy\n",
    "! pip install astropy\n",
    "! pip install photutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T08:50:34.989840Z",
     "start_time": "2021-06-27T08:50:34.977575Z"
    },
    "id": "VSqSitqK2YWE"
   },
   "outputs": [],
   "source": [
    "# Importing all necessary modules\n",
    "import os\n",
    "import glob\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.io import fits\n",
    "from astropy.stats import sigma_clipped_stats\n",
    "import photutils\n",
    "import astroscrappy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u1lUPsid2YWF"
   },
   "source": [
    "Where is your data sitting? For this notebook, keep all of the given files in the folder 'data' in the directory where you're running this notebook.\n",
    "Directory structure should be:\n",
    "\n",
    "- CurrentWorkingDirectory <br>\n",
    "    - data <br>\n",
    "        - bias <br>\n",
    "        - flats <br>\n",
    "        - science <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T08:51:14.657782Z",
     "start_time": "2021-06-27T08:51:14.639807Z"
    },
    "id": "YjLMwkAn2lNY"
   },
   "source": [
    "Optional:- Mounting google drive to import data files directly\n",
    "\n",
    "<br>\n",
    "from google.colab import drive <br>\n",
    "drive.mount('/content/drive', force_remount=True)   <br>\n",
    "\n",
    "cwd = \"/content/drive/MyDrive\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T08:52:52.878750Z",
     "start_time": "2021-06-27T08:52:52.860231Z"
    },
    "id": "pHcb4MnH2YWF"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f62244e4c1be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mscience_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcwd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'science'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'reduced'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#creates new folder in \\content\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mreduced_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcwd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'reduced'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive'"
     ]
    }
   ],
   "source": [
    "# Assigning names for all folders\n",
    "bias_path = os.path.join(cwd,'data','bias')\n",
    "flat_path = os.path.join(cwd,'data','flats')\n",
    "science_path = os.path.join(cwd,'data','science')\n",
    "\n",
    "os.chdir(cwd)\n",
    "os.mkdir('reduced') #creates new folder in \\content\n",
    "reduced_path = os.path.join(cwd,'reduced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T8Cw5-_z2YWG"
   },
   "outputs": [],
   "source": [
    "# A simple function to check whether some directory exists or not. Useful while using these scripts on new data. \n",
    "def do_path_check(path_list):\n",
    "    a_exist = [f for f in path_list if os.path.isdir(f)]\n",
    "    not_exist=list(set(a_exist) ^ set(path_list))\n",
    "    print(\"The following directories exists:\\n {} \\n\".format(a_exist))\n",
    "    if len(not_exist) > 0:\n",
    "        print(\"Please check the path you have given for {}.\\n \\nIt does not exist!!! \\n\".format(not_exist))\n",
    "        return\n",
    "    else:\n",
    "        print(\"All paths exist. \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0SFoPOJ52YWG"
   },
   "outputs": [],
   "source": [
    "def make_folder_check(path):\n",
    "    if os.path.exists(path):\n",
    "        print(\"{} Directory exists.\".format(path.split(\"/\")[-1]))\n",
    "    else:\n",
    "        print(\"{} does not exist.\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BVnO-Om-2YWG"
   },
   "outputs": [],
   "source": [
    "make_folder_check(bias_path)\n",
    "make_folder_check(flat_path)\n",
    "make_folder_check(science_path)\n",
    "# make_folder_check(reduced_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b15JD4Hd2YWH"
   },
   "outputs": [],
   "source": [
    "# Making a list for the available data. Useful reference. \n",
    "bias_list = glob.glob(bias_path +'/*.fits')\n",
    "flat_list = glob.glob(flat_path +'/*.fits')\n",
    "sci_list = glob.glob(science_path +'/*wcs.fits')\n",
    "print(\"Number of bias frames avaialble: {}\".format(len(bias_list)))\n",
    "print(\"Number of flat frames avaialble: {}\".format(len(flat_list)))\n",
    "print(\"Number of science frames avaialble: {}\".format(len(sci_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQTTwqop2YWH"
   },
   "source": [
    "\n",
    "# Getting started with fits files\n",
    "\n",
    "Most of the astronomical images are stored in the form of Flexible Image Transport System (FITS) files. FITS files provide a great option for storing the observational metadata as well as pixel data. With the multi-extension functionality these can be used to store multiple headers and data araays in a single file. \n",
    "\n",
    "More info about fits file system can be found here: https://fits.gsfc.nasa.gov/fits_primer.html\n",
    "\n",
    "Let's spend a few minutes with fits file handling. We will be using a bias image for this purpose (nothing special about a bias image, this will work with any other image as well). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s60DDqed2YWI"
   },
   "outputs": [],
   "source": [
    "test_hdu = fits.open(bias_list[0])\n",
    "# Let's check the hdu info\n",
    "test_hdu.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3bI0PQTY2YWJ"
   },
   "source": [
    "\n",
    "### HDUs\n",
    "Header/Data Units (HDUs) contains both header and data. There can be multiple extensions which can contain multiple headers and multiple data arrays. All the relevent information about the data is stored in headers of the hdu. \n",
    "\n",
    "As we can see in the output of the last cell, there is just one extension i.e. PrimaryHDU. Let's go ahead and access the data and header stored there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fVUz74TZ2YWJ"
   },
   "outputs": [],
   "source": [
    "test_data = test_hdu[0].data\n",
    "test_header = test_hdu[0].header\n",
    "test_hdu.close() # Not necessary, but always a good practice to close the hdu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t_zYW2MF2YWK"
   },
   "outputs": [],
   "source": [
    "test_header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9xpNwXGQ2YWK"
   },
   "outputs": [],
   "source": [
    "# Printing the first 3 entries in the header:\n",
    "\n",
    "\n",
    "# You can also print individual key values as following:\n",
    "\n",
    "x_len = \n",
    "y_len = \n",
    "\n",
    "print(\"\\nThe give image size is {} x {} pixels\".format(x_len, y_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8k_zs3WE2YWK"
   },
   "source": [
    "# Exercise 1\n",
    "\n",
    "Also try to get the following parameters of the image: \n",
    "`DATE-OBS, EXPTIME, CCD-TEMP, IMAGETYP, ALTITUDE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jvOoSf8A2YWK"
   },
   "outputs": [],
   "source": [
    "# Let's see the basic image statistics.\n",
    "mean = \n",
    "median = \n",
    "std = \n",
    "max_data = \n",
    "min_data = \n",
    "print(\"max: {%0.2f}\"%(max_data))\n",
    "print(\"min : {%0.2f}\"%(min_data))\n",
    "print(\"median: {%0.2f}\"%(median))\n",
    "print(\"mean: {%0.2f}\"%(mean))\n",
    "print(\"std: {%0.2f}\"%(std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCAVOlAH2YWL"
   },
   "source": [
    "# Visualising the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q4TG3Om_2YWM"
   },
   "outputs": [],
   "source": [
    "# Let's plot a histogram to understand this distribution\n",
    "# Plot a histogram with a 10-sigma cut from the median. Add a vertical line to display the median as well.  \n",
    "image_hist = \n",
    "plt.xlim()\n",
    "plt.axvline()\n",
    "plt.xlabel('Counts')\n",
    "plt.ylabel('Number of Pixels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KOKvcNL62YWM"
   },
   "source": [
    "Now let's have a actual look at the image itself using matplotlib tools. \n",
    "This can be done easily using `plt.imshow`. But before that let's set up the matplotlib for nicer visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q1eIhpjt2YWM"
   },
   "outputs": [],
   "source": [
    "plt.rc('axes', labelsize=14)\n",
    "plt.rc('axes', titlesize=16)\n",
    "plt.rc('axes', titleweight='bold')\n",
    "plt.rc('axes', labelweight='bold')\n",
    "plt.rc('font', family='sans-serif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S_e_6Qtt2YWN"
   },
   "outputs": [],
   "source": [
    "# Display the image with a 'gray' colormap\n",
    "\n",
    "fig = plt.figure(figsize = (10,10))\n",
    "\n",
    "plt.imshow()\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hselyrEt2YWN"
   },
   "source": [
    "Ahh!!! this is certainly not what you were expecting, right? <br>\n",
    "So now what we can do to improve the visibility of image? \n",
    "Hint: look at the colorbar scale and statstics of image. \n",
    "\n",
    "Try to manipulate the image display style. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rKpKYOaw2YWN"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (10,10))\n",
    "\n",
    "plt.imshow()\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nyQtGOXu2YWN"
   },
   "outputs": [],
   "source": [
    "# Combining median frame to create a master frame\n",
    "\n",
    "def median_combine_frames(img_list):\n",
    "    x_len = fits.getval(img_list[0], 'NAXIS2')\n",
    "    y_len = fits.getval(img_list[0], 'NAXIS1')\n",
    "    n_images = len(img_list)\n",
    "    all_frames = np.empty((n_images, x_len, y_len))\n",
    "    for i in range(n_images):\n",
    "        all_frames[i,:,:] = fits.open(img_list[i])[0].data\n",
    "    master_img = np.median(all_frames, axis=0) \n",
    "\n",
    "    return master_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UxS_LwrC2YWO"
   },
   "source": [
    "Now create a Master Bias! Don't forget to understand every line in the function above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T0lj52Du2YWP"
   },
   "outputs": [],
   "source": [
    "# Creating a Master Bias \n",
    "master_bias = \n",
    "plt.figure(figsize = (10,10))\n",
    "plt.imshow()\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uruZ_lor2YWQ"
   },
   "source": [
    "# AWESOME! This looks good.\n",
    "If not, ask your tutor where you are going wrong. \n",
    "\n",
    "Did you notice that the earlier image required us to set scales manually, while the masterbias image shows good scaling automatically? \n",
    "What changed? \n",
    "Are you never required to set scales for a masterbias?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_DA5EzYX2YWR"
   },
   "source": [
    "# Let's make a masterflat.\n",
    "There are a couple of things you should keep in mind. \n",
    "\n",
    "We use flat frames to correct for pixel response. This response function depends on the wavlength of light and hence depends on which filter we used for that image. So, it is required that we make masterflats for individual filters in practice. You all need not to worry though, we have given you all images of same filter and you can always verify. You already know how to check that.  <br>\n",
    "\n",
    "The other thing you should think about is that do you need to correct flats using masterbias before combining ?\n",
    "\n",
    "# So, this bring us this the next exercise !!!\n",
    "\n",
    "Let's make a masterflat in same manner as we have made a masterbias frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yjGAcNck2YWR"
   },
   "outputs": [],
   "source": [
    "# Read the flat header & data \n",
    "\n",
    "flat_hdu = \n",
    "flat_data = \n",
    "flat_header = \n",
    "flat_hdu.close() # not necessary. But, again, always a good practice to close the hdu. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JWuahKGG2YWR"
   },
   "source": [
    "\n",
    "# Exercise\n",
    "Print stats of all flats to verify that they are useable or not. We will discard the 'bad' flats if necessary. Find out what the median counts of all flats are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rYvLjEb_2YWR"
   },
   "outputs": [],
   "source": [
    "Median_Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ija3qMx32YWS"
   },
   "source": [
    "After verifying that all flats are good, we will now combine them to create a master-flat. You have already combined bias images to create a master-bias. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6N8iRUB12YWT"
   },
   "outputs": [],
   "source": [
    "def flat_combine():\n",
    "    return master_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O2Ang0Bu2YWT"
   },
   "outputs": [],
   "source": [
    "\n",
    "master_flat = flat_combine()\n",
    "print(np.median(master_flat))\n",
    "mean_f, median_f, std_f = sigma_clipped_stats(master_flat)\n",
    "\n",
    "fig = plt.figure(figsize = (10,10))\n",
    "plt.imshow(master_flat, vmin = median_f - 5*std_f, vmax = median_f + 5*std_f)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dry15U_f2YWU"
   },
   "source": [
    "Are you able to see some pattern in the image? Well!! if you look carefully, there are 2 patterns in this image.\n",
    "\n",
    "Can you tell us what could be the possible reasons behind these patterns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "000vZvIH2YWU"
   },
   "outputs": [],
   "source": [
    "## Set the unexposed pixels to NaN, and display again\n",
    "\n",
    "master_flat_norm = np.copy(master_flat)*1.0   # Use 'copy' to preserve the original masterFlat\n",
    "\n",
    "# Set a mask for all pixels where the counts are less than 0.8\n",
    "        \n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow()\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UlsBHSUE2YWU"
   },
   "source": [
    "# Finally!\n",
    "\n",
    "The Bias and flat corrections - The big step that we have been working for so far!\n",
    "\n",
    "Use the correction equation to get a reduced science image. Then save this new reduced science image. \n",
    "Don't forget to add headers that say you have corrected for bias and flats. This is an important step that will help you understand the importance of headers. It also acts like a documentation for all analysis done on the file. Very useful for your own use and especially when someone else accesses your data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AMNtDbiy2YWV"
   },
   "outputs": [],
   "source": [
    "for i in range(len(sci_list)):\n",
    "        # Read in the FITS data from the science image\n",
    "        sci_hdu = \n",
    "        sci_data = \n",
    "        sci_header = \n",
    "        \n",
    "        \n",
    "        fbcr_data =       \n",
    "        \n",
    "        # Write it to a new FITS file   \n",
    "        new_hdu =                     \n",
    "        \n",
    "        # Save the reduced image to a fits file\n",
    "        try:\n",
    "            new_hdu.header.remove('BZERO')\n",
    "            new_hdu.header.remove('BSCALE')\n",
    "        except:\n",
    "            print(\"No BZERO, BSCALE keyword found \")\n",
    "        fbcr_filename = sci_list[i].split(\"/\")[-1].replace('fits','fb.fits')\n",
    "        new_hdu.writeto(os.path.join(reduced_path, fbcr_filename), overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2mwgg6Nr2YWV"
   },
   "source": [
    "## But how does it look like?\n",
    "Let's plot and find out!\n",
    "\n",
    "So, plot all 4 reduced science images to visualize the corrections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "InrRndD62YWV"
   },
   "outputs": [],
   "source": [
    "fbproc_list = glob.glob(reduced_path + \"/*fb.fits\") # list of all flat, bias corrected files. \n",
    "\n",
    "plt.figure(figsize=(14,14))\n",
    "for i in range(len(fbproc_list)):\n",
    "        \n",
    "        \n",
    "        plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0DvV5aY2YWV"
   },
   "source": [
    "### Now on to the final stage of the pre-processing: Cosmic ray removal! \n",
    "\n",
    "You might already know that cosmic rays are chared particals protons and nuclei. These charged particles move through space with velocity nearly the speed of light. These produce a shower of secondary particles as soon as they hit the upper layer of the earth's atmosphere. Charged particles intract in different way as compared to the photons. They deposit most of their energy in very small area and can have different profiles in CCD image. We use this cretireia to differentiate them from astrophysical sources.\n",
    "\n",
    "`lacosmic` is one of the best available algorithm to identify various types of cosmic ray hits. We are going to use the python package `astroscrappy` (https://astroscrappy.readthedocs.io/en/latest/) which is based on the above mentioned algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZZ4_0fYs2YWW"
   },
   "outputs": [],
   "source": [
    "Gain = 1.6 # electrons / ADU\n",
    "read_noise = 14.0 # electrons\n",
    "saturation = 96000 # electrons\n",
    "\n",
    "for i in range(len(fbproc_list)):\n",
    "    fbproc_hdu = fits.open(os.path.join(reduced_path, fbproc_list[i]))\n",
    "    fbproc_data = fbproc_hdu[0].data\n",
    "    fbproc_header = fbproc_hdu[0].header\n",
    "    new_data = np.copy(fbproc_data)\n",
    "    cosmic_mask, clean_data = astroscrappy.detect_cosmics(new_data, gain=Gain, readnoise=read_noise, satlevel=saturation)\n",
    "    print('{} pixels are affected by cosmic rays for file {}'.format(np.sum(cosmic_mask), fbproc_list[i].split(\"/\")[-1]))\n",
    "    proc_data = clean_data / Gain\n",
    "    \n",
    "    if np.any(master_flat < 0.8):\n",
    "        proc_data[mask] = float('NaN') # Correcting for the vignetting region \n",
    "\n",
    "    proc_header = fbproc_header\n",
    "    proc_header.add_history('Cosmic ray removed') \n",
    "    cleaned_image = fbproc_list[i].replace(\"fb.fits\", \"proc.fits\")\n",
    "    fits.writeto(os.path.join(reduced_path, cleaned_image), proc_data, proc_header, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5Z7wgl-2YWW"
   },
   "source": [
    "Plot the cleaned_image below and see whether the vignetting region is still present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DJ5i1e6r2YWW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97rIsCHL2YWW"
   },
   "source": [
    "Let's check if the cosmic ray correction worked or not. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rkqeney52YWW"
   },
   "outputs": [],
   "source": [
    "_, m1, std1 = sigma_clipped_stats(fbproc_data[2812-20:2812+20, 1396-20:1396+20])\n",
    "_, m2, std2 = sigma_clipped_stats(proc_data[2812-20:2812+20, 1396-20:1396+20])\n",
    "\n",
    "plt.imshow(fbproc_data[2812-20:2812+20, 1396-20:1396+20], vmin = m1-5*std1, vmax = m1 + 5*std1)\n",
    "plt.show()\n",
    "plt.imshow(proc_data[2812-20:2812+20, 1396-20:1396+20], vmin = m2-5*std2, vmax = m2 + 5*std2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pm88dahc2YWX"
   },
   "source": [
    "The above mention three step (bias correction, flat fielding and cosmic ray removal) are most common operation of pre-processing. Although, depending on each telescope facility there might be other operations can be done in pre-processing. Such corrections are not applicable for general use as these operation are usually telescope faciity specific. \n",
    "\n",
    "Sometimes astronomers try to take multiple short exposures and stack the images to get better S/N ratio. This also help in getting rid of comsic ray removal step. Spend a minute thinking why we can skip cosmic ray removal part after stacking (see home exercise at the end of notebook) the image?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rix0eGuM2YWX"
   },
   "source": [
    "After all these corrections, we usually perform one more step i.e. solving image for astrometry.\n",
    "You must be thinking what does that means?  \n",
    "\n",
    "When we take images through telescope facility, the camera stores photons numbers in each pixels. It has no information of where the photons are coming from. Although the telescope pointing software usaully gives rough estimate of direction, but it is impossible to associate the photons of each pixel to aspecific part of sky. During solving images for astrometry we establish a unique realtion between image pixels and sky coordinates. This allos us to identify sources in the image.\n",
    "\n",
    "This step require astrometry.net's `solve-field` engine to solve the images (you can download it from here [https://astrometry.net/use.html], if you are interested) with the help of index files. These indexfiles are rather large ~ 50GB in total. We have provided you astrometry-solved images to avoid this heavy download. Alternate option is to upload image on the publically available online server of astrometry.net here: [https://nova.astrometry.net/upload] and download the solved images from there.  \n",
    "*Important Note*:- You might want to login first and then change a few advanced settings in order to keep your data private. Otherwise uploaded images will be publicaly available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xsCV4l_2YWX"
   },
   "source": [
    "# Take Home Exercise\n",
    "\n",
    "All these images are processed and ready to use for science use. As we have multiple science images for the same field, we can try to stack (a process of combining images) them to make an image with better S/N ratio. The most comoon used software is SWarp. Here is the documentation for SWarp [https://www.astromatic.net/pubsvn/software/swarp/trunk/doc/swarp.pdf]. you can go into detais of how to combine images and try yourself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fred9boS2YWX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nURBpV5-2YWX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Basics of Image Reduction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
